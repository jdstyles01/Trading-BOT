{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37019dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import kagglehub\n",
    "import yfinance as yf\n",
    "from openai import OpenAI\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b88742f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PaperConfig:\n",
    "    start_date_train: str = \"1998-01-01\"\n",
    "    end_date_train: str = \"2015-12-31\"\n",
    "    start_date_test: str = \"2016-01-01\"\n",
    "    end_date_test: str = \"2019-12-31\"\n",
    "\n",
    "    pca_components: int = 4          # best model in Table II\n",
    "    hidden_size: int = 32\n",
    "    dropout: float = 0.3\n",
    "    batch_size: int = 64\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 500\n",
    "\n",
    "    # For reproducibility\n",
    "    seed: int = 42\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "cfg = PaperConfig()\n",
    "set_seed(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec405750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spy_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a DataFrame with index = Date, columns = ['open', 'close', 'volume'].\n",
    "    \"\"\"\n",
    "    # TODO: replace with your actual loading logic\n",
    "    df = yf.download(\"^GSPC\", start=\"1998-01-01\", end=\"2021-12-31\")\n",
    "    df = df.rename(columns=str.lower)[['open', 'close', 'volume']]\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    return df\n",
    "    raise NotImplementedError\n",
    "\n",
    "def load_dxy_data() -> pd.DataFrame:\n",
    "    dxy_raw = yf.download(\"DX-Y.NYB\", start=\"1998-01-01\", end=\"2021-12-31\", auto_adjust=False)\n",
    "    df = dxy_raw.rename(columns=str.lower)[['open', 'close', 'volume']]\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    return df.rename(columns={\n",
    "        'open': 'dxy_open',\n",
    "        'close': 'dxy_close',\n",
    "        'volume': 'dxy_volume',\n",
    "    })\n",
    "\n",
    "\n",
    "def load_yield_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download 3M and 10Y yields from Yahoo and return a flat DataFrame:\n",
    "    index = Date, columns = ['yield_3m', 'yield_10y'].\n",
    "    \"\"\"\n",
    "    # ^IRX = 13-week T-bill (proxy for 3M), ^TNX = 10Y treasury\n",
    "    tickers = {\"^IRX\": \"yield_3m\", \"^TNX\": \"yield_10y\"}\n",
    "\n",
    "    raw = yf.download(list(tickers.keys()),\n",
    "                      start=\"1998-01-01\",\n",
    "                      end=\"2021-12-31\",\n",
    "                      auto_adjust=False)\n",
    "\n",
    "    # raw.columns: MultiIndex (field, ticker). We want Close prices.\n",
    "    close = raw[\"Close\"].copy()        # columns: ['^IRX','^TNX']\n",
    "\n",
    "    close.columns = [tickers[c] for c in close.columns]\n",
    "    close.index = close.index.tz_localize(None)\n",
    "\n",
    "    return close      # columns: ['yield_3m', 'yield_10y']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2625756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wsj_headlines() -> pd.DataFrame:\n",
    "    df_headlines = pd.read_csv(r\"C:\\Users\\diego\\Dropbox\\Proyectos\\wsj_headlines.csv\")\n",
    "    df_headlines = df_headlines.rename(columns={\"Date\": \"date\", \"Headline\": \"headline\", \"Category\": \"category\"})\n",
    "    df_headlines.index = pd.to_datetime(df_headlines[\"date\"])\n",
    "    df_headlines = df_headlines[[\"date\",\"headline\", \"category\"]]\n",
    "    return df_headlines\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96358fe3-2b61-4169-81e4-84c802a71013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el cliente una sola vez (usa OPENAI_API_KEY del entorno)\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92513e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_headline(text: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Embebe un titular (o cualquier texto corto) usando text-embedding-3-small\n",
    "    y devuelve un vector np.ndarray de float32.\n",
    "    \"\"\"\n",
    "    resp = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    )\n",
    "    # resp.data[0].embedding es la lista de floats que devuelve la API\n",
    "    return np.array(resp.data[0].embedding, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0341291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_headline_embeddings(headlines_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    headlines_df: columns ['date', 'headline']\n",
    "    Returns DataFrame indexed by date with a column 'embedding' that holds np.array(1536,)\n",
    "    and also expanded numeric columns if desired.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for _, row in headlines_df.iterrows():\n",
    "        emb = embed_headline(row[\"headline\"])\n",
    "        rows.append({\n",
    "            \"date\": row[\"date\"],\n",
    "            \"embedding\": emb\n",
    "        })\n",
    "    emb_df = pd.DataFrame(rows)\n",
    "    emb_df[\"date\"] = pd.to_datetime(emb_df[\"date\"]).dt.normalize()\n",
    "    return emb_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "700334f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_one_headline_per_day(emb_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    emb_df: ['date', 'embedding']\n",
    "    Returns DataFrame indexed by date, exactly one embedding per date.\n",
    "    \"\"\"\n",
    "    # Randomly choose one row per date\n",
    "    emb_df = emb_df.sample(frac=1.0, random_state=cfg.seed)  # shuffle\n",
    "    one_per_day = emb_df.drop_duplicates(subset=\"date\", keep=\"first\")\n",
    "    one_per_day = one_per_day.set_index(\"date\").sort_index()\n",
    "    return one_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5680251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca_on_train(embeddings: pd.DataFrame, cfg: PaperConfig) -> Tuple[PCA, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    embeddings: DataFrame indexed by date, column 'embedding' (np.array)\n",
    "    Returns fitted PCA and DataFrame with columns ['pca_1', ..., 'pca_k'].\n",
    "    \"\"\"\n",
    "    # Convert list/array column into 2D matrix\n",
    "    X = np.stack(embeddings[\"embedding\"].values)  # shape (n_days, 1536)\n",
    "\n",
    "    pca = PCA(n_components=cfg.pca_components, random_state=cfg.seed)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    cols = [f\"headline_pca_{i+1}\" for i in range(cfg.pca_components)]\n",
    "    pca_df = pd.DataFrame(X_pca, index=embeddings.index, columns=cols)\n",
    "    return pca, pca_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a532178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(embeddings: pd.DataFrame, pca: PCA, cfg: PaperConfig) -> pd.DataFrame:\n",
    "    X = np.stack(embeddings[\"embedding\"].values)\n",
    "    X_pca = pca.transform(X)\n",
    "    cols = [f\"headline_pca_{i+1}\" for i in range(cfg.pca_components)]\n",
    "    pca_df = pd.DataFrame(X_pca, index=embeddings.index, columns=cols)\n",
    "    return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7dca326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_merged_dataset(cfg: PaperConfig) -> pd.DataFrame:\n",
    "    spy = load_spy_data()\n",
    "    dxy = load_dxy_data()\n",
    "    yld = load_yield_data()\n",
    "\n",
    "    spy.index = pd.to_datetime(spy.index).tz_localize(None)\n",
    "    dxy.index = pd.to_datetime(dxy.index).tz_localize(None)\n",
    "    yld.index = pd.to_datetime(yld.index).tz_localize(None)\n",
    "\n",
    "    spy = spy.sort_index()\n",
    "    spy[\"log_ret_next\"] = np.log(spy[\"close\"].shift(-1) / spy[\"close\"])\n",
    "\n",
    "    df = spy[[\"open\", \"close\", \"volume\", \"log_ret_next\"]].copy()\n",
    "    df = df.join(dxy[[\"dxy_open\"]], how=\"left\")\n",
    "    df = df.join(yld[[\"yield_3m\", \"yield_10y\"]], how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "    # 3) Headline embeddings (assume precomputed)\n",
    "    # Suppose you already have a DataFrame 'headline_embs'\n",
    "    # indexed by date with column 'embedding'.\n",
    "    # Here we just show the flow.\n",
    "    raise NotImplementedError(\"You need to prepare headline_embs DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecb672dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_headline_pca_features(df: pd.DataFrame,\n",
    "                              headline_embs: pd.DataFrame,\n",
    "                              cfg: PaperConfig) -> Tuple[pd.DataFrame, PCA]:\n",
    "    \"\"\"\n",
    "    df: price+macro DataFrame indexed by date\n",
    "    headline_embs: DataFrame indexed by date with column 'embedding' (np.array)\n",
    "    \"\"\"\n",
    "    # Align headline embeddings with df index\n",
    "    emb_aligned = headline_embs.reindex(df.index).dropna(subset=[\"embedding\"])\n",
    "\n",
    "    # Fit PCA on training period only\n",
    "    train_mask = (emb_aligned.index >= cfg.start_date_train) & (emb_aligned.index <= cfg.end_date_train)\n",
    "    emb_train = emb_aligned[train_mask]\n",
    "\n",
    "    pca, _ = fit_pca_on_train(emb_train, cfg)\n",
    "\n",
    "    # Transform full (train + test) embeddings\n",
    "    pca_full = apply_pca(emb_aligned, pca, cfg)   # index = dates where we have embeddings\n",
    "\n",
    "    # Safely align PCA features to df's index and assign column by column\n",
    "    pca_full_aligned = pca_full.reindex(df.index)\n",
    "\n",
    "    for col in pca_full_aligned.columns:\n",
    "        df[col] = pca_full_aligned[col]\n",
    "\n",
    "    return df, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "473c2d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    If df.columns is a MultiIndex like ('open', '^gspc'),\n",
    "    flatten it to just 'open' (first element of the tuple).\n",
    "    \"\"\"\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df = df.copy()\n",
    "        df.columns = [c[0] if isinstance(c, tuple) else c for c in df.columns]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e82fcb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_dataset(df: pd.DataFrame, cfg: PaperConfig) -> Tuple[pd.DataFrame, pd.DataFrame, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Split into train/test, handle missing data, and scale features.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- FLATTEN COLUMNS ---\n",
    "    df = flatten_columns(df)\n",
    "\n",
    "    # --- Debug: show columns & check target ---\n",
    "    print(\"DEBUG finalize_dataset | columns:\", list(df.columns)[:20])\n",
    "    print(\"DEBUG finalize_dataset | 'log_ret_next' present?\", \"log_ret_next\" in df.columns)\n",
    "\n",
    "    # Ensure target column exists\n",
    "    if \"log_ret_next\" not in df.columns:\n",
    "        if \"close\" not in df.columns:\n",
    "            raise ValueError(\n",
    "                \"finalize_dataset: df has no 'log_ret_next' and no 'close' to compute it from. \"\n",
    "                f\"Columns are: {list(df.columns)}\"\n",
    "            )\n",
    "        df = df.sort_index()\n",
    "        df[\"log_ret_next\"] = np.log(df[\"close\"].shift(-1) / df[\"close\"])\n",
    "        print(\"DEBUG finalize_dataset | created 'log_ret_next' from 'close'.\")\n",
    "\n",
    "    # Drop rows without target\n",
    "    df = df.dropna(subset=[\"log_ret_next\"])\n",
    "\n",
    "    # Define base feature columns\n",
    "    feature_cols = [\n",
    "        \"open\", \"close\", \"volume\",\n",
    "        \"dxy_open\", \"yield_3m\", \"yield_10y\",\n",
    "    ]\n",
    "\n",
    "    # Add PCA columns if present\n",
    "    pca_cols = [c for c in df.columns\n",
    "                if isinstance(c, str) and c.startswith(\"headline_pca_\")]\n",
    "    feature_cols += pca_cols\n",
    "\n",
    "    # Drop rows with missing features (or you could impute)\n",
    "    df = df.dropna(subset=feature_cols)\n",
    "\n",
    "    # Train/test split by date\n",
    "    train_mask = (df.index >= cfg.start_date_train) & (df.index <= cfg.end_date_train)\n",
    "    test_mask  = (df.index >= cfg.start_date_test)  & (df.index <= cfg.end_date_test)\n",
    "\n",
    "    train_df = df[train_mask].copy()\n",
    "    test_df  = df[test_mask].copy()\n",
    "\n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df[\"log_ret_next\"].values\n",
    "\n",
    "    X_test  = test_df[feature_cols].values\n",
    "    y_test  = test_df[\"log_ret_next\"].values\n",
    "\n",
    "    # Scale features on train only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    # Store scaled arrays back into the DataFrames\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        train_df[col] = X_train_scaled[:, i]\n",
    "        test_df[col]  = X_test_scaled[:, i]\n",
    "\n",
    "    return train_df, test_df, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d242a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeIndependentDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: list, target_col: str = \"log_ret_next\"):\n",
    "        self.X = df[feature_cols].values.astype(np.float32)\n",
    "        self.y = df[target_col].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21f1f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_size: int = 32, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Xavier init\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b2f528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error in percent.\n",
    "    \"\"\"\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    # avoid division by zero\n",
    "    denom = np.where(denom == 0, 1e-8, denom)\n",
    "    return 100.0 * np.mean(np.abs(y_true - y_pred) / denom * 2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87ed0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                cfg: PaperConfig,\n",
    "                device: str = \"cpu\") -> Dict[str, float]:\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # Optional: simple validation loss tracking\n",
    "        if (epoch + 1) % 50 == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for X_val, y_val in val_loader:\n",
    "                    X_val = X_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    preds = model(X_val)\n",
    "                    loss = criterion(preds, y_val)\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{cfg.epochs} \"\n",
    "                  f\"Train MSE: {np.mean(train_losses):.6f} \"\n",
    "                  f\"Val MSE: {np.mean(val_losses):.6f}\")\n",
    "\n",
    "    # Return model; metrics computed separately\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f92a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module,\n",
    "                   df: pd.DataFrame,\n",
    "                   feature_cols: list,\n",
    "                   target_col: str = \"log_ret_next\",\n",
    "                   device: str = \"cpu\") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    X = df[feature_cols].values.astype(np.float32)\n",
    "    y_true = df[target_col].values.astype(np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.from_numpy(X).to(device)\n",
    "        y_pred = model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    s = smape(y_true, y_pred)\n",
    "\n",
    "    return {\"MSE\": mse, \"R2\": r2, \"SMAPE\": s}, y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76e6c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_best_paper_model(cfg: PaperConfig,\n",
    "                         headline_embs: pd.DataFrame | None) -> None:\n",
    "    \"\"\"\n",
    "    headline_embs:\n",
    "      - DataFrame indexed by date with column 'embedding' (np.array), or\n",
    "      - None to skip news features and use only price + macro.\n",
    "    \"\"\"\n",
    "    # 1) Base SPX + macro data\n",
    "    spy = load_spy_data()\n",
    "    dxy = load_dxy_data()\n",
    "    yld = load_yield_data()\n",
    "\n",
    "    # Normalize indices to tz-naive DateTimeIndex\n",
    "    for frame in (spy, dxy, yld):\n",
    "        if isinstance(frame.index, pd.MultiIndex):\n",
    "            frame.index = frame.index.get_level_values(0)\n",
    "        frame.index = pd.to_datetime(frame.index).tz_localize(None)\n",
    "\n",
    "    spy = spy.sort_index()\n",
    "    spy[\"log_ret_next\"] = np.log(spy[\"close\"].shift(-1) / spy[\"close\"])\n",
    "\n",
    "    # Base df\n",
    "    df = spy[[\"open\", \"close\", \"volume\", \"log_ret_next\"]].copy()\n",
    "\n",
    "    # Add macro via aligned reindex (no joins)\n",
    "    df[\"dxy_open\"]  = dxy[\"dxy_open\"].reindex(df.index)\n",
    "    df[\"yield_3m\"]  = yld[\"yield_3m\"].reindex(df.index)\n",
    "    df[\"yield_10y\"] = yld[\"yield_10y\"].reindex(df.index)\n",
    "\n",
    "    # 2) Add PCA headline features if we have them\n",
    "    if headline_embs is not None:\n",
    "        df, pca = add_headline_pca_features(df, headline_embs, cfg)\n",
    "\n",
    "    # 3) Finalize dataset (split + scaling)\n",
    "    # PCA cols will be [] if we didn't add news\n",
    "    pca_cols = [c for c in df.columns\n",
    "                if isinstance(c, str) and c.startswith(\"headline_pca_\")]\n",
    "\n",
    "    feature_cols = [\"open\", \"close\", \"volume\",\n",
    "                    \"dxy_open\", \"yield_3m\", \"yield_10y\"] + pca_cols\n",
    "\n",
    "    print(\"DEBUG run_best_paper_model | feature_cols:\", feature_cols)\n",
    "\n",
    "    train_df, test_df, scaler = finalize_dataset(df, cfg)\n",
    "\n",
    "    # 4) Build PyTorch datasets/loaders\n",
    "    train_mask_inner = train_df.index < \"2014-01-01\"\n",
    "    val_mask_inner   = train_df.index >= \"2014-01-01\"\n",
    "\n",
    "    train_inner = train_df[train_mask_inner]\n",
    "    val_inner   = train_df[val_mask_inner]\n",
    "\n",
    "    train_dataset = TimeIndependentDataset(train_inner, feature_cols)\n",
    "    val_dataset   = TimeIndependentDataset(val_inner, feature_cols)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "    # 5) Model\n",
    "    input_dim = len(feature_cols)\n",
    "    model = FFNN(input_dim=input_dim,\n",
    "                 hidden_size=cfg.hidden_size,\n",
    "                 dropout=cfg.dropout)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 6) Train\n",
    "    train_model(model, train_loader, val_loader, cfg, device=device)\n",
    "\n",
    "    # 7) Evaluate\n",
    "    train_metrics, _, _ = evaluate_model(model, train_df, feature_cols, device=device)\n",
    "    test_metrics,  _, _ = evaluate_model(model,  test_df,  feature_cols, device=device)\n",
    "\n",
    "    print(\"Train metrics:\", train_metrics)\n",
    "    print(\"Test  metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0583aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_headlines = load_wsj_headlines()\n",
    "emb_df = build_headline_embeddings(df_headlines[[\"date\", \"headline\"]])\n",
    "one_per_day = select_one_headline_per_day(emb_df)\n",
    "headline_embs = one_per_day  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8945b70d-c447-4fad-b894-6df4ede2398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_43452\\4064750746.py:6: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(\"^GSPC\", start=\"1998-01-01\", end=\"2021-12-31\")\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  2 of 2 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG run_best_paper_model | feature_cols: ['open', 'close', 'volume', 'dxy_open', 'yield_3m', 'yield_10y']\n",
      "DEBUG finalize_dataset | columns: ['open', 'close', 'volume', 'log_ret_next', 'dxy_open', 'yield_3m', 'yield_10y', 'headline_pca_1', 'headline_pca_2', 'headline_pca_3', 'headline_pca_4']\n",
      "DEBUG finalize_dataset | 'log_ret_next' present? True\n",
      "Epoch 1/500 Train MSE: 0.170364 Val MSE: 0.142930\n",
      "Epoch 50/500 Train MSE: 0.000215 Val MSE: 0.000259\n",
      "Epoch 100/500 Train MSE: 0.000176 Val MSE: 0.000173\n",
      "Epoch 150/500 Train MSE: 0.000176 Val MSE: 0.000094\n",
      "Epoch 200/500 Train MSE: 0.000172 Val MSE: 0.000093\n",
      "Epoch 250/500 Train MSE: 0.000168 Val MSE: 0.000129\n",
      "Epoch 300/500 Train MSE: 0.000169 Val MSE: 0.000263\n",
      "Epoch 350/500 Train MSE: 0.000170 Val MSE: 0.000408\n",
      "Epoch 400/500 Train MSE: 0.000167 Val MSE: 0.000329\n",
      "Epoch 450/500 Train MSE: 0.000164 Val MSE: 0.000468\n",
      "Epoch 500/500 Train MSE: 0.000166 Val MSE: 0.000400\n",
      "Train metrics: {'MSE': 0.00019128543, 'R2': -0.18252894599133906, 'SMAPE': 166.49456024169922}\n",
      "Test  metrics: {'MSE': 0.002988201, 'R2': -45.05162820203449, 'SMAPE': 181.95003271102905}\n"
     ]
    }
   ],
   "source": [
    "run_best_paper_model(cfg, headline_embs=headline_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e5e04-cb46-4398-8d15-ba544eb1b129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37019dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import kagglehub\n",
    "import yfinance as yf\n",
    "from openai import OpenAI\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88742f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PaperConfig:\n",
    "    start_date_train: str = \"1998-01-01\"\n",
    "    end_date_train: str = \"2015-12-31\"\n",
    "    start_date_test: str = \"2016-01-01\"\n",
    "    end_date_test: str = \"2019-12-31\"\n",
    "\n",
    "    pca_components: int = 4          # best model in Table II\n",
    "    hidden_size: int = 32\n",
    "    dropout: float = 0.3\n",
    "    batch_size: int = 64\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 500\n",
    "\n",
    "    # For reproducibility\n",
    "    seed: int = 42\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "cfg = PaperConfig()\n",
    "set_seed(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec405750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spy_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a DataFrame with index = Date, columns = ['open', 'close', 'volume'].\n",
    "    \"\"\"\n",
    "    # TODO: replace with your actual loading logic\n",
    "    df = yf.download(\"^GSPC\", start=\"1998-01-01\", end=\"2021-12-31\")\n",
    "    df = df.rename(columns=str.lower)[['open', 'close', 'volume']]\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    return df\n",
    "    raise NotImplementedError\n",
    "\n",
    "def load_dxy_data() -> pd.DataFrame:\n",
    "    dxy_raw = yf.download(\"DX-Y.NYB\", start=\"1998-01-01\", end=\"2021-12-31\", auto_adjust=False)\n",
    "    df = dxy_raw.rename(columns=str.lower)[['open', 'close', 'volume']]\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    return df.rename(columns={\n",
    "        'open': 'dxy_open',\n",
    "        'close': 'dxy_close',\n",
    "        'volume': 'dxy_volume',\n",
    "    })\n",
    "\n",
    "\n",
    "def load_yield_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download 3M and 10Y yields from Yahoo and return a flat DataFrame:\n",
    "    index = Date, columns = ['yield_3m', 'yield_10y'].\n",
    "    \"\"\"\n",
    "    # ^IRX = 13-week T-bill (proxy for 3M), ^TNX = 10Y treasury\n",
    "    tickers = {\"^IRX\": \"yield_3m\", \"^TNX\": \"yield_10y\"}\n",
    "\n",
    "    raw = yf.download(list(tickers.keys()),\n",
    "                      start=\"1998-01-01\",\n",
    "                      end=\"2021-12-31\",\n",
    "                      auto_adjust=False)\n",
    "\n",
    "    # raw.columns: MultiIndex (field, ticker). We want Close prices.\n",
    "    close = raw[\"Close\"].copy()        # columns: ['^IRX','^TNX']\n",
    "\n",
    "    close.columns = [tickers[c] for c in close.columns]\n",
    "    close.index = close.index.tz_localize(None)\n",
    "\n",
    "    return close      # columns: ['yield_3m', 'yield_10y']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2625756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wsj_headlines() -> pd.DataFrame:\n",
    "    df_headlines = pd.read_csv(r\"C:\\Users\\diego\\Dropbox\\Proyectos\\wsj_headlines.csv\")\n",
    "    df_headlines = df_headlines.rename(columns={\"Date\": \"date\", \"Headline\": \"headline\", \"Category\": \"category\"})\n",
    "    df_headlines.index = pd.to_datetime(df_headlines[\"date\"])\n",
    "    df_headlines = df_headlines[[\"date\",\"headline\", \"category\"]]\n",
    "    return df_headlines\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76233783",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"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\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ce7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"  # or load from .env\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92513e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_headline(text: str) -> np.ndarray:\n",
    "    resp = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    )\n",
    "    return np.array(resp.data[0].embedding, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0341291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_headline_embeddings(headlines_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    headlines_df: columns ['date', 'headline']\n",
    "    Returns DataFrame indexed by date with a column 'embedding' that holds np.array(1536,)\n",
    "    and also expanded numeric columns if desired.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for _, row in headlines_df.iterrows():\n",
    "        emb = embed_headline(row[\"headline\"])\n",
    "        rows.append({\n",
    "            \"date\": row[\"date\"],\n",
    "            \"embedding\": emb\n",
    "        })\n",
    "    emb_df = pd.DataFrame(rows)\n",
    "    emb_df[\"date\"] = pd.to_datetime(emb_df[\"date\"]).dt.normalize()\n",
    "    return emb_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700334f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_one_headline_per_day(emb_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    emb_df: ['date', 'embedding']\n",
    "    Returns DataFrame indexed by date, exactly one embedding per date.\n",
    "    \"\"\"\n",
    "    # Randomly choose one row per date\n",
    "    emb_df = emb_df.sample(frac=1.0, random_state=cfg.seed)  # shuffle\n",
    "    one_per_day = emb_df.drop_duplicates(subset=\"date\", keep=\"first\")\n",
    "    one_per_day = one_per_day.set_index(\"date\").sort_index()\n",
    "    return one_per_day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5680251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca_on_train(embeddings: pd.DataFrame, cfg: PaperConfig) -> Tuple[PCA, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    embeddings: DataFrame indexed by date, column 'embedding' (np.array)\n",
    "    Returns fitted PCA and DataFrame with columns ['pca_1', ..., 'pca_k'].\n",
    "    \"\"\"\n",
    "    # Convert list/array column into 2D matrix\n",
    "    X = np.stack(embeddings[\"embedding\"].values)  # shape (n_days, 1536)\n",
    "\n",
    "    pca = PCA(n_components=cfg.pca_components, random_state=cfg.seed)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    cols = [f\"headline_pca_{i+1}\" for i in range(cfg.pca_components)]\n",
    "    pca_df = pd.DataFrame(X_pca, index=embeddings.index, columns=cols)\n",
    "    return pca, pca_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a532178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(embeddings: pd.DataFrame, pca: PCA, cfg: PaperConfig) -> pd.DataFrame:\n",
    "    X = np.stack(embeddings[\"embedding\"].values)\n",
    "    X_pca = pca.transform(X)\n",
    "    cols = [f\"headline_pca_{i+1}\" for i in range(cfg.pca_components)]\n",
    "    pca_df = pd.DataFrame(X_pca, index=embeddings.index, columns=cols)\n",
    "    return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7dca326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_merged_dataset(cfg: PaperConfig) -> pd.DataFrame:\n",
    "    spy = load_spy_data()\n",
    "    dxy = load_dxy_data()\n",
    "    yld = load_yield_data()\n",
    "\n",
    "    spy.index = pd.to_datetime(spy.index).tz_localize(None)\n",
    "    dxy.index = pd.to_datetime(dxy.index).tz_localize(None)\n",
    "    yld.index = pd.to_datetime(yld.index).tz_localize(None)\n",
    "\n",
    "    spy = spy.sort_index()\n",
    "    spy[\"log_ret_next\"] = np.log(spy[\"close\"].shift(-1) / spy[\"close\"])\n",
    "\n",
    "    df = spy[[\"open\", \"close\", \"volume\", \"log_ret_next\"]].copy()\n",
    "    df = df.join(dxy[[\"dxy_open\"]], how=\"left\")\n",
    "    df = df.join(yld[[\"yield_3m\", \"yield_10y\"]], how=\"left\")\n",
    "\n",
    "\n",
    "\n",
    "    # 3) Headline embeddings (assume precomputed)\n",
    "    # Suppose you already have a DataFrame 'headline_embs'\n",
    "    # indexed by date with column 'embedding'.\n",
    "    # Here we just show the flow.\n",
    "    raise NotImplementedError(\"You need to prepare headline_embs DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecb672dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_headline_pca_features(df: pd.DataFrame,\n",
    "                              headline_embs: pd.DataFrame,\n",
    "                              cfg: PaperConfig) -> Tuple[pd.DataFrame, PCA]:\n",
    "    # Align headline embeddings with df index\n",
    "    # Inner join on dates where both exist\n",
    "    emb_aligned = headline_embs.reindex(df.index).dropna(subset=[\"embedding\"])\n",
    "\n",
    "    # Fit PCA on training period only\n",
    "    train_mask = (emb_aligned.index >= cfg.start_date_train) & (emb_aligned.index <= cfg.end_date_train)\n",
    "    emb_train = emb_aligned[train_mask]\n",
    "\n",
    "    pca, pca_train = fit_pca_on_train(emb_train, cfg)\n",
    "\n",
    "    # Transform full (train + test) embeddings\n",
    "    pca_full = apply_pca(emb_aligned, pca, cfg)\n",
    "\n",
    "    # Join PCA features into df\n",
    "    df = df.join(pca_full, how=\"left\")\n",
    "\n",
    "    return df, pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e82fcb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_dataset(df: pd.DataFrame, cfg: PaperConfig) -> Tuple[pd.DataFrame, pd.DataFrame, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Split into train/test, handle missing data, and scale features.\n",
    "    \"\"\"\n",
    "    # Drop rows without target or with missing PCA features\n",
    "    df = df.dropna(subset=[\"log_ret_next\"])\n",
    "\n",
    "    # Define feature columns\n",
    "    feature_cols = [\n",
    "        \"open\", \"close\", \"volume\",\n",
    "        \"dxy_open\", \"yield_3m\", \"yield_10y\",\n",
    "    ]\n",
    "    # Add PCA columns\n",
    "    pca_cols = [c for c in df.columns if c.startswith(\"headline_pca_\")]\n",
    "    feature_cols += pca_cols\n",
    "\n",
    "    # Drop rows with missing features (or you can impute)\n",
    "    df = df.dropna(subset=feature_cols)\n",
    "\n",
    "    # Train/test split by date\n",
    "    train_mask = (df.index >= cfg.start_date_train) & (df.index <= cfg.end_date_train)\n",
    "    test_mask = (df.index >= cfg.start_date_test) & (df.index <= cfg.end_date_test)\n",
    "\n",
    "    train_df = df[train_mask].copy()\n",
    "    test_df = df[test_mask].copy()\n",
    "\n",
    "    X_train = train_df[feature_cols].values\n",
    "    y_train = train_df[\"log_ret_next\"].values\n",
    "\n",
    "    X_test = test_df[feature_cols].values\n",
    "    y_test = test_df[\"log_ret_next\"].values\n",
    "\n",
    "    # Scale features on train only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Store scaled arrays back in DataFrames for later convenience\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        train_df[col] = X_train_scaled[:, i]\n",
    "        test_df[col] = X_test_scaled[:, i]\n",
    "\n",
    "    return train_df, test_df, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d242a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeIndependentDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, feature_cols: list, target_col: str = \"log_ret_next\"):\n",
    "        self.X = df[feature_cols].values.astype(np.float32)\n",
    "        self.y = df[target_col].values.astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21f1f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_size: int = 32, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Xavier init\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b2f528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Symmetric Mean Absolute Percentage Error in percent.\n",
    "    \"\"\"\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred))\n",
    "    # avoid division by zero\n",
    "    denom = np.where(denom == 0, 1e-8, denom)\n",
    "    return 100.0 * np.mean(np.abs(y_true - y_pred) / denom * 2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87ed0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                val_loader: DataLoader,\n",
    "                cfg: PaperConfig,\n",
    "                device: str = \"cpu\") -> Dict[str, float]:\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # Optional: simple validation loss tracking\n",
    "        if (epoch + 1) % 50 == 0 or epoch == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for X_val, y_val in val_loader:\n",
    "                    X_val = X_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    preds = model(X_val)\n",
    "                    loss = criterion(preds, y_val)\n",
    "                    val_losses.append(loss.item())\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{cfg.epochs} \"\n",
    "                  f\"Train MSE: {np.mean(train_losses):.6f} \"\n",
    "                  f\"Val MSE: {np.mean(val_losses):.6f}\")\n",
    "\n",
    "    # Return model; metrics computed separately\n",
    "    return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f92a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model: nn.Module,\n",
    "                   df: pd.DataFrame,\n",
    "                   feature_cols: list,\n",
    "                   target_col: str = \"log_ret_next\",\n",
    "                   device: str = \"cpu\") -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    X = df[feature_cols].values.astype(np.float32)\n",
    "    y_true = df[target_col].values.astype(np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.from_numpy(X).to(device)\n",
    "        y_pred = model(X_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    s = smape(y_true, y_pred)\n",
    "\n",
    "    return {\"MSE\": mse, \"R2\": r2, \"SMAPE\": s}, y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76e6c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_best_paper_model(cfg: PaperConfig,\n",
    "                         headline_embs: pd.DataFrame | None) -> None:\n",
    "    \"\"\"\n",
    "    headline_embs: DataFrame indexed by date with column 'embedding' (np.array),\n",
    "    or None to skip news features and use only price + macro.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # 1) Build base SPX + macro dataset\n",
    "    spy = load_spy_data()\n",
    "    dxy = load_dxy_data()\n",
    "    yld = load_yield_data()\n",
    "\n",
    "    # Normalize indices to simple tz-naive DatetimeIndex\n",
    "    for frame in (spy, dxy, yld):\n",
    "        if isinstance(frame.index, pd.MultiIndex):\n",
    "            frame.index = frame.index.get_level_values(0)\n",
    "        frame.index = pd.to_datetime(frame.index).tz_localize(None)\n",
    "\n",
    "    spy = spy.sort_index()\n",
    "    spy[\"log_ret_next\"] = np.log(spy[\"close\"].shift(-1) / spy[\"close\"])\n",
    "\n",
    "    # Base df\n",
    "    df = spy[[\"open\", \"close\", \"volume\", \"log_ret_next\"]].copy()\n",
    "\n",
    "    # Avoid join/merge: align columns by reindexing\n",
    "    df[\"dxy_open\"]  = dxy[\"dxy_open\"].reindex(df.index)\n",
    "    df[\"yield_3m\"]  = yld[\"yield_3m\"].reindex(df.index)\n",
    "    df[\"yield_10y\"] = yld[\"yield_10y\"].reindex(df.index)\n",
    "\n",
    "    # 2) Add PCA headline features (optional)\n",
    "    if headline_embs is not None:\n",
    "        df, pca = add_headline_pca_features(df, headline_embs, cfg)\n",
    "\n",
    "    # 3) Finalize dataset (split + scaling)\n",
    "    # Safely select PCA columns (skip non-string column names if any)\n",
    "    pca_cols = [c for c in df.columns\n",
    "                if isinstance(c, str) and c.startswith(\"headline_pca_\")]\n",
    "\n",
    "    feature_cols = [\"open\", \"close\", \"volume\",\n",
    "                    \"dxy_open\", \"yield_3m\", \"yield_10y\"] + pca_cols\n",
    "\n",
    "    train_df, test_df, scaler = finalize_dataset(df, cfg)\n",
    "\n",
    "    # 4) Build PyTorch datasets/loaders\n",
    "    # Simple inner train/val split: last 2 years of train as val\n",
    "    train_mask_inner = train_df.index < \"2014-01-01\"\n",
    "    val_mask_inner   = train_df.index >= \"2014-01-01\"\n",
    "\n",
    "    train_inner = train_df[train_mask_inner]\n",
    "    val_inner   = train_df[val_mask_inner]\n",
    "\n",
    "    train_dataset = TimeIndependentDataset(train_inner, feature_cols)\n",
    "    val_dataset   = TimeIndependentDataset(val_inner, feature_cols)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False)\n",
    "\n",
    "    # 5) Model\n",
    "    input_dim = len(feature_cols)\n",
    "    model = FFNN(input_dim=input_dim,\n",
    "                 hidden_size=cfg.hidden_size,\n",
    "                 dropout=cfg.dropout)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 6) Train\n",
    "    train_model(model, train_loader, val_loader, cfg, device=device)\n",
    "\n",
    "    # 7) Evaluate on train + test sets\n",
    "    train_metrics, _, _ = evaluate_model(model, train_df, feature_cols, device=device)\n",
    "    test_metrics,  _, _ = evaluate_model(model,  test_df,  feature_cols, device=device)\n",
    "\n",
    "    print(\"Train metrics:\", train_metrics)\n",
    "    print(\"Test  metrics:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0583aa07",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df_headlines \u001b[38;5;241m=\u001b[39m load_wsj_headlines()\n\u001b[1;32m----> 2\u001b[0m emb_df \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_headline_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_headlines\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheadline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m one_per_day \u001b[38;5;241m=\u001b[39m select_one_headline_per_day(emb_df)\n\u001b[0;32m      4\u001b[0m headline_embs \u001b[38;5;241m=\u001b[39m one_per_day  \n",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m, in \u001b[0;36mbuild_headline_embeddings\u001b[1;34m(headlines_df)\u001b[0m\n\u001b[0;32m      7\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m headlines_df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m----> 9\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43membed_headline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheadline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: emb\n\u001b[0;32m     13\u001b[0m     })\n\u001b[0;32m     14\u001b[0m emb_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m, in \u001b[0;36membed_headline\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21membed_headline\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m----> 2\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-3-small\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(resp\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39membedding, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\resources\\embeddings.py:132\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    127\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\openai\\_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "df_headlines = load_wsj_headlines()\n",
    "emb_df = build_headline_embeddings(df_headlines[[\"date\", \"headline\"]])\n",
    "one_per_day = select_one_headline_per_day(emb_df)\n",
    "headline_embs = one_per_day  \n",
    "\n",
    "run_best_paper_model(cfg, headline_embs=headline_embs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
